{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO74X31bajMyzLhGuNSb8MU"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing dependencies"
      ],
      "metadata": {
        "id": "iXumFuTIhDQG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vZIwt7nDqti"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import re,string,unicodedata\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem.porter import PorterStemmer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading the dataset"
      ],
      "metadata": {
        "id": "L9lpAIPxhCLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_df=pd.read_csv(\"news_dataset.csv\", engine='python',encoding='utf-8', error_bad_lines=False)\n",
        "news_df\n"
      ],
      "metadata": {
        "id": "nPmtGWqHEnIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_text=news_df['text']\n",
        "news_text"
      ],
      "metadata": {
        "id": "gkN_7YrFZWWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning the text"
      ],
      "metadata": {
        "id": "vY-KHoImhROU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_df['text']=news_df['text'].apply(str)"
      ],
      "metadata": {
        "id": "MHpMNbHXY2lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_df['text']= news_df['text'].apply((lambda y:re.sub(\"http://\\S+\",\" \", y)))\n",
        "news_df['text']= news_df['text'].apply((lambda x:re.sub(\"\\@\", \" \",x.lower())))\n",
        "news_df['text']= news_df['text'].apply((lambda z:re.sub(r'[^\\w\\s]', '', z)))\n",
        "news_df['text']= news_df['text'].apply((lambda w:re.sub(\"[0-9]\", \"\", w)))\n",
        "news_df['text']= news_df['text'].apply((lambda v:re.sub(\"\\n\", \"\", v)))\n",
        "\n",
        "news_df['text']"
      ],
      "metadata": {
        "id": "uAPai6zERjd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_df['text'][1]"
      ],
      "metadata": {
        "id": "jHXEYxrdgmhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For removing hindi characters from the dataset\n",
        "news_df['text'] = news_df['text'].apply(lambda q:re.sub(\"([^\\x900-\\x97F])+\",\" \",q))"
      ],
      "metadata": {
        "id": "jLog7RWJFWmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_df['text'][1] #hindi characters successfully removed "
      ],
      "metadata": {
        "id": "QRwC82IUg1lW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop = stopwords.words('english')\n",
        "news_df['text']= news_df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop]))\n",
        "news_df['text']\n"
      ],
      "metadata": {
        "id": "yQbkMPeXS9Qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(news_df.text.apply(word_tokenize))"
      ],
      "metadata": {
        "id": "Jdq_k_D2V-I2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()\n",
        "corpus = []\n",
        "review = [ps.stem(word) for word in news_df['text']if not word in stopwords.words('english')]\n",
        "review = ' '.join(review)\n",
        "corpus.append(review)"
      ],
      "metadata": {
        "id": "M2fiI-2NwyMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels=news_df.label\n",
        "labels.head()"
      ],
      "metadata": {
        "id": "ZrOadNVkhJfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and testing"
      ],
      "metadata": {
        "id": "bSZsmoTIhYGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test=train_test_split(news_df['text'], labels, test_size=0.2, random_state=7)"
      ],
      "metadata": {
        "id": "XojU_vihxdKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize a TfidfVectorizer\n",
        "tfidf_vectorizer=TfidfVectorizer(stop_words='english', max_df=0.7)\n",
        "#Fit and transform train set, transform test set\n",
        "tfidf_train=tfidf_vectorizer.fit_transform(x_train) \n",
        "tfidf_test=tfidf_vectorizer.transform(x_test)"
      ],
      "metadata": {
        "id": "BOuxh2wPhfjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "pac=PassiveAggressiveClassifier(max_iter=100)\n",
        "pac.fit(tfidf_train,y_train)\n",
        "#Predict on the test set and calculate accuracy\n",
        "y_pred=pac.predict(tfidf_test)\n",
        "score=accuracy_score(y_test,y_pred)\n",
        "print(f'Accuracy: {round(score*100,2)}%')"
      ],
      "metadata": {
        "id": "Ckqoo_EshkFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(y_test,y_pred, labels=['FAKE','REAL'])"
      ],
      "metadata": {
        "id": "pdjSScxJh2tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "review = re.sub('[^a-zA-Z]', ' ', news_df['text'][1])\n",
        "review = review.lower()\n",
        "review = review.split() \n",
        "review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
        "review = ' '.join(review)\n",
        "# Vectorization \n",
        "val = tfidf_vectorizer.transform([review]).toarray()\n",
        "# Predict \n",
        "pac.predict(val)"
      ],
      "metadata": {
        "id": "qjcBUMx6mxNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pickling the model"
      ],
      "metadata": {
        "id": "esBlljH7hvfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pickle.dump(pac, open('model.pkl', 'wb'))\n",
        "pickle.dump(tfidf_vectorizer, open('tfidfvect.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "v7R2G7L-oqPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joblib_model = pickle.load(open('model.pkl', 'rb'))\n",
        "joblib_vect = pickle.load(open('tfidfvect.pkl', 'rb'))\n",
        "val_pkl = joblib_vect.transform([review]).toarray()\n",
        "joblib_model.predict(val_pkl)"
      ],
      "metadata": {
        "id": "aYyXwdXEo3wQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}